{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = '../models/model_lite'\n",
    "input_vocab = '../data/vocab_lite.json'\n",
    "\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.losses = []\n",
    "        \n",
    "        # Khởi tạo trọng số với Xavier Initialization\n",
    "        limit = np.sqrt(6 / (vocab_size + embedding_dim))\n",
    "        self.W1 = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))  # |V| x d\n",
    "        self.W2 = np.random.uniform(-limit, limit, (embedding_dim, vocab_size))  # d x |V|\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Stabilize computation\n",
    "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    def forward(self, center_idx):\n",
    "        \"\"\"\n",
    "        Forward pass to compute probabilities of context words.\n",
    "        center_idx: Indices of the center words (batch_size).\n",
    "        \"\"\"\n",
    "        center_vectors = self.W1[center_idx]  # batch_size x d\n",
    "        logits = np.dot(center_vectors, self.W2)  # batch_size x |V|\n",
    "        probabilities = self.softmax(logits)  # batch_size x |V|\n",
    "        return probabilities, center_vectors\n",
    "\n",
    "    def backward(self, probabilities, center_idx, context_idx):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients and update weights for a batch.\n",
    "        probabilities: Softmax probabilities (batch_size x |V|).\n",
    "        center_idx: Indices of the center words (batch_size).\n",
    "        context_idx: Indices of the context words (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = len(center_idx)\n",
    "        \n",
    "        # One-hot encoding for context words\n",
    "        targets = np.zeros_like(probabilities)  # batch_size x |V|\n",
    "        targets[np.arange(batch_size), context_idx] = 1\n",
    "\n",
    "        # Error between predicted probabilities and target\n",
    "        error = probabilities - targets  # batch_size x |V|\n",
    "\n",
    "        # Gradients for W2 and W1\n",
    "        grad_W2 = np.dot(self.W1[center_idx].T, error)  # d x |V|\n",
    "        grad_W1 = np.dot(error, self.W2.T)  # batch_size x d\n",
    "\n",
    "        # Gradient aggregation for W1 (sum gradients for same indices)\n",
    "        unique_idx, inverse_idx = np.unique(center_idx, return_inverse=True)\n",
    "        grouped_gradients = np.zeros((len(unique_idx), self.embedding_dim))\n",
    "        np.add.at(grouped_gradients, inverse_idx, grad_W1)\n",
    "\n",
    "        # Cập nhật W1\n",
    "        self.W1[unique_idx] -= self.learning_rate * grouped_gradients\n",
    "        self.W2 -= self.learning_rate * grad_W2\n",
    "\n",
    "    def train(self, word_pairs, vocab, epochs=10, batch_size=64):\n",
    "        \"\"\"\n",
    "        Train the Skip-gram model.\n",
    "        word_pairs: List of (center_word, context_word) tuples.\n",
    "        vocab: Vocabulary mapping word to index.\n",
    "        \"\"\"\n",
    "        print(\"Training Skip-gram model...\")\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            np.random.shuffle(word_pairs)\n",
    "            batches = [word_pairs[i:i + batch_size] for i in range(0, len(word_pairs), batch_size)]\n",
    "\n",
    "            for batch in tqdm(batches, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                # Extract indices for batch\n",
    "                center_idx = np.array([vocab[center] for center, _ in batch])\n",
    "                context_idx = np.array([vocab[context] for _, context in batch])\n",
    "\n",
    "                # Forward pass\n",
    "                probabilities, _ = self.forward(center_idx)\n",
    "\n",
    "                # Compute loss (negative log likelihood)\n",
    "                batch_loss = -np.sum(np.log(probabilities[np.arange(len(context_idx)), context_idx]))\n",
    "                total_loss += batch_loss\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(probabilities, center_idx, context_idx)\n",
    "            \n",
    "            avg_loss = total_loss / len(word_pairs)\n",
    "            self.losses.append(avg_loss)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def get_embedding(self, word, vocab):\n",
    "        \"\"\"\n",
    "        Retrieve the embedding vector for a given word.\n",
    "        \"\"\"\n",
    "        return self.W1[vocab[word]]\n",
    "\n",
    "    def cosine_similarity(self, word1, word2, vocab):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two words.\n",
    "        \"\"\"\n",
    "        vec1 = self.get_embedding(word1, vocab)\n",
    "        vec2 = self.get_embedding(word2, vocab)\n",
    "        \n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        similarity = np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)\n",
    "        return similarity\n",
    "    \n",
    "    def save(self, path_prefix):\n",
    "        np.save(f\"{path_prefix}_W1.npy\", self.W1)\n",
    "        np.save(f\"{path_prefix}_W2.npy\", self.W2)\n",
    "        print(f\"Model saved to {path_prefix}_W1.npy and {path_prefix}_W2.npy\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path_prefix, vocab_size, embedding_dim, learning_rate=0.01):\n",
    "        model = cls(vocab_size, embedding_dim, learning_rate)\n",
    "        model.W1 = np.load(f\"{path_prefix}_W1.npy\")\n",
    "        model.W2 = np.load(f\"{path_prefix}_W2.npy\")\n",
    "        print(f\"Model loaded from {path_prefix}_W1.npy and {path_prefix}_W2.npy\")\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_vocab, 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/model_lite_W1.npy and ../models/model_lite_W2.npy\n",
      "Model saved to ../models/model_lite_W1.npy and ../models/model_lite_W2.npy\n"
     ]
    }
   ],
   "source": [
    "model = SkipGram.load(model_prefix, len(vocab), embedding_dim)\n",
    "model.save(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between \"công_ty\" and \"doanh_nghiệp\": 0.6762245560598559\n",
      "Similarity between \"bệnh_viện\" and \"y_tế\": 0.585382316890465\n",
      "Similarity between 'mua' and 'bán': 0.613483139249227\n",
      "Similarity between \"tối\" and \"xấu\": 0.21036727345092737\n",
      "Similarity between \"đại_dương\" and \"hạnh_phúc\": 0.11719318433262459\n",
      "Similarity between \"giáo_viên\" and \"cửa_sổ\": -0.03127936671792931\n",
      "Similarity between \"ông\" and \"cha\": 0.4748734326576516\n"
     ]
    }
   ],
   "source": [
    "similarity = model.cosine_similarity('công_ty', 'doanh_nghiệp', vocab)\n",
    "print('Similarity between \"công_ty\" and \"doanh_nghiệp\":', similarity)\n",
    "\n",
    "similarity = model.cosine_similarity('bệnh_viện', 'y_tế', vocab)\n",
    "print('Similarity between \"bệnh_viện\" and \"y_tế\":', similarity)\n",
    "\n",
    "similarity = model.cosine_similarity('mua', 'bán', vocab)\n",
    "print(\"Similarity between 'mua' and 'bán':\", similarity)\n",
    "\n",
    "similarity = model.cosine_similarity('tối', 'xấu', vocab)\n",
    "print('Similarity between \"tối\" and \"xấu\":', similarity)\n",
    "\n",
    "similarity = model.cosine_similarity('đại_dương', 'hạnh_phúc', vocab)\n",
    "print('Similarity between \"đại_dương\" and \"hạnh_phúc\":', similarity)\n",
    "\n",
    "similarity = model.cosine_similarity('giáo_viên', 'cửa_sổ', vocab)\n",
    "print('Similarity between \"giáo_viên\" and \"cửa_sổ\":', similarity)\n",
    "\n",
    "similarity = model.cosine_similarity('ông', 'cha', vocab)\n",
    "print('Similarity between \"ông\" and \"cha\":', similarity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
