{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.3-cp312-cp312-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\program files\\python\\python12\\lib\\site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nguye\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nguye\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nguye\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.0 MB 645.7 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.5/8.0 MB 645.7 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/8.0 MB 685.3 kB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.0/8.0 MB 729.5 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.3/8.0 MB 808.5 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 1.6/8.0 MB 873.8 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 1.8/8.0 MB 883.1 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.1/8.0 MB 917.5 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.4/8.0 MB 952.0 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.6/8.0 MB 993.4 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.9/8.0 MB 1.0 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 3.4/8.0 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 3.9/8.0 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.5/8.0 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.0/8.0 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.8/8.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.6/8.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 1.8 MB/s eta 0:00:00\n",
      "Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.0/2.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 5.0 MB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Using cached pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.7 matplotlib-3.10.0 pillow-11.0.0 pyparsing-3.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = '../data/processed_data_lite.json'\n",
    "input_vocab = '../data/vocab_lite.json'\n",
    "output_model_prefix = '../models/model_lite'\n",
    "\n",
    "embedding_dim = 100  # Kích thước vector nhúng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tạo các cặp Skip-gram pairs\n",
    "def skipgram_pairs(corpus, window_size=2):\n",
    "    word_pairs = []\n",
    "    for sentence in corpus:\n",
    "        words_in_sentence = sentence.split()\n",
    "        for i, word in enumerate(words_in_sentence):\n",
    "            # Tạo cặp từ cho từ trung tâm và từ ngữ cảnh\n",
    "            context = words_in_sentence[max(i - window_size, 0):i] + words_in_sentence[i + 1:i + window_size + 1]\n",
    "            for context_word in context:\n",
    "                word_pairs.append((word, context_word))\n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.losses = []\n",
    "        \n",
    "        # Khởi tạo trọng số với Xavier Initialization\n",
    "        limit = np.sqrt(6 / (vocab_size + embedding_dim))\n",
    "        self.W1 = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))  # |V| x d\n",
    "        self.W2 = np.random.uniform(-limit, limit, (embedding_dim, vocab_size))  # d x |V|\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Stabilize computation\n",
    "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    def forward(self, center_idx):\n",
    "        \"\"\"\n",
    "        Forward pass to compute probabilities of context words.\n",
    "        center_idx: Indices of the center words (batch_size).\n",
    "        \"\"\"\n",
    "        center_vectors = self.W1[center_idx]  # batch_size x d\n",
    "        logits = np.dot(center_vectors, self.W2)  # batch_size x |V|\n",
    "        probabilities = self.softmax(logits)  # batch_size x |V|\n",
    "        return probabilities, center_vectors\n",
    "\n",
    "    def backward(self, probabilities, center_idx, context_idx):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients and update weights for a batch.\n",
    "        probabilities: Softmax probabilities (batch_size x |V|).\n",
    "        center_idx: Indices of the center words (batch_size).\n",
    "        context_idx: Indices of the context words (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = len(center_idx)\n",
    "        \n",
    "        # One-hot encoding for context words\n",
    "        targets = np.zeros_like(probabilities)  # batch_size x |V|\n",
    "        targets[np.arange(batch_size), context_idx] = 1\n",
    "\n",
    "        # Error between predicted probabilities and target\n",
    "        error = probabilities - targets  # batch_size x |V|\n",
    "\n",
    "        # Gradients for W2 and W1\n",
    "        grad_W2 = np.dot(self.W1[center_idx].T, error)  # d x |V|\n",
    "        grad_W1 = np.dot(error, self.W2.T)  # batch_size x d\n",
    "\n",
    "        # Gradient aggregation for W1 (sum gradients for same indices)\n",
    "        unique_idx, inverse_idx = np.unique(center_idx, return_inverse=True)\n",
    "        grouped_gradients = np.zeros((len(unique_idx), self.embedding_dim))\n",
    "        np.add.at(grouped_gradients, inverse_idx, grad_W1)\n",
    "\n",
    "        # Cập nhật W1\n",
    "        self.W1[unique_idx] -= self.learning_rate * grouped_gradients\n",
    "        self.W2 -= self.learning_rate * grad_W2\n",
    "\n",
    "    def train(self, word_pairs, vocab, epochs=10, batch_size=64):\n",
    "        \"\"\"\n",
    "        Train the Skip-gram model.\n",
    "        word_pairs: List of (center_word, context_word) tuples.\n",
    "        vocab: Vocabulary mapping word to index.\n",
    "        \"\"\"\n",
    "        print(\"Training Skip-gram model...\")\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            np.random.shuffle(word_pairs)\n",
    "            batches = [word_pairs[i:i + batch_size] for i in range(0, len(word_pairs), batch_size)]\n",
    "\n",
    "            for batch in tqdm(batches, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                # Extract indices for batch\n",
    "                center_idx = np.array([vocab[center] for center, _ in batch])\n",
    "                context_idx = np.array([vocab[context] for _, context in batch])\n",
    "\n",
    "                # Forward pass\n",
    "                probabilities, _ = self.forward(center_idx)\n",
    "\n",
    "                # Compute loss (negative log likelihood)\n",
    "                batch_loss = -np.sum(np.log(probabilities[np.arange(len(context_idx)), context_idx]))\n",
    "                total_loss += batch_loss\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(probabilities, center_idx, context_idx)\n",
    "            \n",
    "            avg_loss = total_loss / len(word_pairs)\n",
    "            self.losses.append(avg_loss)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def get_embedding(self, word, vocab):\n",
    "        \"\"\"\n",
    "        Retrieve the embedding vector for a given word.\n",
    "        \"\"\"\n",
    "        return self.W1[vocab[word]]\n",
    "\n",
    "    def cosine_similarity(self, word1, word2, vocab):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two words.\n",
    "        \"\"\"\n",
    "        vec1 = self.get_embedding(word1, vocab)\n",
    "        vec2 = self.get_embedding(word2, vocab)\n",
    "        \n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        similarity = np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)\n",
    "        return similarity\n",
    "    \n",
    "    def save(self, path_prefix):\n",
    "        np.save(f\"{path_prefix}_W1.npy\", self.W1)\n",
    "        np.save(f\"{path_prefix}_W2.npy\", self.W2)\n",
    "        print(f\"Model saved to {path_prefix}_W1.npy and {path_prefix}_W2.npy\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path_prefix, vocab_size, embedding_dim, learning_rate=0.01):\n",
    "        model = cls(vocab_size, embedding_dim, learning_rate)\n",
    "        model.W1 = np.load(f\"{path_prefix}_W1.npy\")\n",
    "        model.W2 = np.load(f\"{path_prefix}_W2.npy\")\n",
    "        print(f\"Model loaded from {path_prefix}_W1.npy and {path_prefix}_W2.npy\")\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    # Tính tích vô hướng giữa vec1 và vec2\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Tính độ dài của từng vector\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Tính cosine similarity\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def word_similarity(word1, word2, vocab, embedding_matrix):\n",
    "    idx1 = vocab[word1]\n",
    "    idx2 = vocab[word2]\n",
    "\n",
    "    vec1 = embedding_matrix[idx1]\n",
    "    vec2 = embedding_matrix[idx2]\n",
    "\n",
    "    return cosine_similarity(vec1, vec2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(input_data, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(input_vocab, 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2964/2964 [00:00<00:00, 3496.73it/s]\n",
      "100%|██████████| 2964/2964 [00:00<00:00, 16084.83it/s]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [skipgram_pairs(doc['contents'], 4) for doc in tqdm(data)]\n",
    "word_pairs = [item for sublist in tqdm(word_pairs) for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Skip-gram model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/13:   0%|          | 7/5892 [00:04<58:15,  1.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SkipGram(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab), embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 77\u001b[0m, in \u001b[0;36mSkipGram.train\u001b[1;34m(self, word_pairs, vocab, epochs, batch_size)\u001b[0m\n\u001b[0;32m     74\u001b[0m probabilities, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(center_idx)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Compute loss (negative log likelihood)\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(probabilities[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontext_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, context_idx]))\n\u001b[0;32m     78\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SkipGram(vocab_size=len(vocab), embedding_dim=embedding_dim, learning_rate=0.01)\n",
    "model.train(word_pairs, vocab, epochs=13, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between \"ông\" and \"cha\": 0.4748734326576516\n",
      "Similarity between \"ông\" and \"mẹ\": 0.28641870964357563\n",
      "Similarity between \"giáo_viên\" and \"buồn_bã\": 0.1674076263902389\n",
      "Similarity between \"hạnh_phúc\" and \"vua\": 0.13152055262311524\n",
      "Similarity between \"học\" and \"giáo_dục\": 0.39925688612246435\n",
      "Similarity between \"anh\" and \"chị\": 0.7177625838393557\n"
     ]
    }
   ],
   "source": [
    "similarity = model.cosine_similarity('ông', 'cha', vocab)\n",
    "print('Similarity between \"ông\" and \"cha\":', similarity)\n",
    "similarity = model.cosine_similarity('ông', 'mẹ', vocab)\n",
    "print('Similarity between \"ông\" and \"mẹ\":', similarity)\n",
    "similarity = model.cosine_similarity('giáo_viên', 'buồn_bã', vocab)\n",
    "print('Similarity between \"giáo_viên\" and \"buồn_bã\":', similarity)\n",
    "similarity = model.cosine_similarity('hạnh_phúc', 'vua', vocab)\n",
    "print('Similarity between \"hạnh_phúc\" and \"vua\":', similarity)\n",
    "similarity = model.cosine_similarity('học', 'giáo_dục', vocab)\n",
    "print('Similarity between \"học\" and \"giáo_dục\":', similarity)\n",
    "similarity = model.cosine_similarity('anh', 'chị', vocab)\n",
    "print('Similarity between \"anh\" and \"chị\":', similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(output_model_prefix)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(output_model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
